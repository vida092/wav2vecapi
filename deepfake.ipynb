{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50903e-39ec-4953-a95c-7488da246f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f7512-3e2d-4dd2-b493-e8f3ed1d0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"ruta\") ## aquí hay que escribir la ruta a la carpeta \"LA\" descargable en  https://www.kaggle.com/datasets/awsaf49/asvpoof-2019-dataset\n",
    "PROTOCOLS_DIR = BASE_DIR / \"ASVspoof2019_LA_cm_protocols\"\n",
    "\n",
    "dev_protocol_file = PROTOCOLS_DIR / \"ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "\n",
    "cols = [\"speaker_id\", \"audio_id\", \"unused\", \"unused2\", \"key\"]\n",
    "\n",
    "df_dev = pd.read_csv(\n",
    "    dev_protocol_file,\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=cols\n",
    ")\n",
    "\n",
    "df_dev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64193c9a-4c12-4dbf-8b04-a588b19f2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_AUDIO_DIR = BASE_DIR / \"ASVspoof2019_LA_dev\" / \"flac\"\n",
    "\n",
    "df_dev[\"path\"] = df_dev[\"audio_id\"].apply(lambda x: DEV_AUDIO_DIR / f\"{x}.flac\")\n",
    "df_dev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d03dcf-533d-4f48-b34a-61363c0cc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = df_dev.iloc[-60]\n",
    "\n",
    "audio, sr = librosa.load(example[\"path\"], sr=16000)\n",
    "print(f\"Tamaño de df_dev es {len(df_dev)} por {len(list(df_dev.columns))}\")\n",
    "print(\"Etiqueta:\", example[\"key\"])\n",
    "print(\"Forma:\", audio.shape)\n",
    "print(\"Sampling rate:\", sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d211d-fb0c-46a9-a035-6e0ca4bccce4",
   "metadata": {},
   "source": [
    "# Wave2Vec XLSR\n",
    "\n",
    "Es un modelo neuronal auto-supervisado para audio, desarrollado por facebook AI, ahora meta.\n",
    "Su funcion principal es convertir audio \"crudo\" en representaciones vectoriales que capturan contenido fonético, esctructura rítmica, timbre características del hablante y artefactos acústicos.\n",
    "\n",
    "## ¿Por qué es útil?\n",
    "Un audio generado por IA puede contener:\n",
    "* artifactos espectrales sutiles\n",
    "* irregularidades en el pitch\n",
    "* formantes no naturales\n",
    "* ruidos no humanos\n",
    "* inconsistencias temporales\n",
    "\n",
    "Wave2Vec ha sido entrenado para diferenciar patrones naturales del habla humana, sus embeings permiten distingir entre humano (bonafide) o spoof (deepfake) sin tener que entrenar desde cero\n",
    "\n",
    "# Cómo funciona?\n",
    "\n",
    "1. Entra audio crudo $x(t)$, donde $x\\in\\mathbb{R}^T$ con $T$ muestras de audio\n",
    "2. Feature Encoder (CNN temporal):\n",
    "    * Primero el audio pasa por 7 capas convolucionales: $$ z =f_{CNN}(x) $$\n",
    "       * Donde stride reduce la resolución temporal y kernels grandes capturan patrones locales, por lo que el resultado es $$z\\in\\mathbb{R}^{L\\times d}$$. Cada \"frame\" de $z$ contiene información acústica, $z$ NO ES EL EMBEDING FINAL\n",
    "# Masking (auto-supervisado)\n",
    "La magia del preentrenamiento es que se ocultan (se enmascaran) bloques aleatorios de frames y el modelo debe predecir qué había ahí. Este comportamiento es parecido a BERT en NLM pero a audio.\n",
    "\n",
    "# Transformer Encoder\n",
    "La salida enmascarada pasa por un gran transformer c:\n",
    "$$c=Transformer(z_{masked})$$\n",
    "\n",
    "El transformer es el que aprende contexto global:\n",
    "* fonemas\n",
    "* ritmo\n",
    "* prosodia\n",
    "* estructura temporal\n",
    "* acento\n",
    "* características de la voz\n",
    "\n",
    "El resultado final entonces es $x\\in\\mathbb{R}^{L\\times H}$ donde $H\\simeq 768-1024$.\n",
    "\n",
    "Este $c$ es el que ya podemos usar como embedding. No necesitamos predecir los tokens contrastivos como en el pre-entrenamiento. Solo usamos el encoder como extractor de características\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6456a-7672-4bd1-b0c4-32bcd65df24d",
   "metadata": {},
   "source": [
    "# Qué representa un embedding de este modelo\n",
    "\n",
    "Cada vector de 1024 dimensiones contiene:\n",
    "* timbre de voz\n",
    "* patrón espectral global\n",
    "* transición fonética\n",
    "* ritmo\n",
    "* calidad de la señal\n",
    "* artefactos de vocoder\n",
    "* distorsiones TTS\n",
    "* ruido y reverberación\n",
    "Todos estos factores son importantes para distinguir deepfakes.\n",
    "\n",
    "Aplicamos, entonces,\n",
    "$$e = \\frac{1}{L}\\sum_{i=1}^{L}c_i$$\n",
    "\n",
    "que es la definición de media.\n",
    "\n",
    "Un solo vector de 1024 representa todo el audio.\n",
    "\n",
    "Básicamente Wav2Vec fue entrenado con decenas de miles de horas de habla humana real, aprendió a reconocer patrones humanos y cuando ve algo \"antinatural\" el embedding es diferente por lo que un clasificador clásico de ML podría separar bonafide vs spoof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c661eb-bc97-446d-814e-f36a5d406aed",
   "metadata": {},
   "source": [
    "# Pipeline a grosso modo\n",
    "Audio crudo → CNN Encoder → Transformer → Embeddings → Clasificador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5451c89-6f11-472b-a1b8-1209e7a178ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar Feature Extractor \n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c5f85-3537-4b16-a4f5-4f9383230d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cargar modelo Wav2Vec2 XLSR\n",
    "# Este modelo produce embeddings de dimensión 1024 por frame.\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\"\n",
    ")\n",
    "\n",
    "model = Wav2Vec2Model.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\"\n",
    ")\n",
    "\n",
    "# MUY IMPORTANTE: mover el modelo COMPLETO a GPU AQUÍ\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Modelo está en:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b155f-c0ec-48de-bc1f-168db8673d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Función para cargar audio y convertirlo en embedding\n",
    "\n",
    "def load_audio(path, target_sr=16000):\n",
    "    audio, sr = librosa.load(path, sr=target_sr)\n",
    "    return audio\n",
    "\n",
    "\n",
    "def extract_embedding(path):\n",
    "    audio = load_audio(path)\n",
    "\n",
    "    # preparar tensores\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Mover tensores al GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward pass en GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Mean pooling\n",
    "    emb = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "\n",
    "    # Pasar a CPU para convertir a numpy\n",
    "    return emb.squeeze().cpu().numpy()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"Usando dispositivo:\", device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No se detectó GPU en PyTorch.\")\n",
    "\n",
    "test_path = df_dev[\"path\"].iloc[0]\n",
    "\n",
    "start = time.time()\n",
    "_ = extract_embedding(test_path)\n",
    "end = time.time()\n",
    "\n",
    "tiempo_gpu = end - start\n",
    "print(f\"Tiempo por audio en GPU: {tiempo_gpu:.4f} segundos\")\n",
    "\n",
    "\n",
    "num_dev = len(df_dev)\n",
    "tiempo_est_dev = num_dev * tiempo_gpu\n",
    "\n",
    "print(f\"Audios en DEV: {num_dev}\")\n",
    "print(f\"Tiempo estimado (DEV): {tiempo_est_dev/60:.2f} minutos\")\n",
    "\n",
    "try:\n",
    "    num_train = len(df_train)\n",
    "    tiempo_est_train = num_train * tiempo_gpu\n",
    "\n",
    "    print(f\"Audios en TRAIN: {num_train}\")\n",
    "    print(f\"Tiempo estimado (TRAIN): {tiempo_est_train/60:.2f} minutos\")\n",
    "\n",
    "except:\n",
    "    print(\"df_train no cargado todavía\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165e02f-0014-417f-a840-c2a996835466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PRUEBA: extraer embedding de un archivo del dataset DEV\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "example_path = df_dev[\"path\"].iloc[0]\n",
    "example_label = df_dev[\"key\"].iloc[0]\n",
    "\n",
    "print(\"Archivo:\", example_path)\n",
    "print(\"Etiqueta:\", example_label)\n",
    "\n",
    "emb = extract_embedding(example_path)\n",
    "\n",
    "print(\"Embedding shape:\", emb.shape)\n",
    "print(\"Primeros valores:\", emb[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2a8bc-175a-4445-86c7-634c7746b1ed",
   "metadata": {},
   "source": [
    "# Generar embeddings para todo el dataset (train/dev)\n",
    "\n",
    "Con esto construiremos:\n",
    "\n",
    "* X_train: matriz de embeddings\n",
    "\n",
    "* y_train: etiquetas bonafide/spoof\n",
    "\n",
    "* X_dev: embeddings del conjunto de validación\n",
    "\n",
    "* y_dev: etiquetas\n",
    "\n",
    "Este paso es indispensable para poder entrenar un clasificador de detección de deepfake.\n",
    "\n",
    "1. Convertir \"bonafide\" → 0 y \"spoof\" → 1\n",
    "\n",
    "2. Crear una función generadora de embeddings por fila del dataframe\n",
    "\n",
    "3. Aplicarlo con progress_apply (tqdm)\n",
    "\n",
    "4. Guardar los embeddings en arrays numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abfb43-6376-4139-a8f6-1f5d11b33d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa0dc0-1745-4d5d-994d-882f4de2bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convertir las etiquetas de texto a etiquetas numéricas\n",
    "\n",
    "# bonafide = 0 (voz humana real)\n",
    "# spoof = 1 (deepfake / TTS / voice conversion)\n",
    "\n",
    "\n",
    "label_map = {\"bonafide\": 0, \"spoof\": 1}\n",
    "\n",
    "df_dev[\"label_num\"] = df_dev[\"key\"].map(label_map)\n",
    "\n",
    "# Verificamos que esté correcto\n",
    "df_dev[df_dev[\"label_num\"] == 1][[\"key\", \"label_num\"]].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61376a-9795-4206-8049-c2cdca6e854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev[df_dev[\"label_num\"] == 0][[\"key\", \"label_num\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44659b-1400-40af-b879-5f89d45ad079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Función auxiliar para aplicar extract_embedding con tqdm\n",
    "# Esta función recibe una fila del DataFrame y usa la columna \"path\"\n",
    "# para extraer el embedding con la función extract_embedding().\n",
    "\n",
    "def get_embedding(row):\n",
    "    try:\n",
    "        return extract_embedding(row[\"path\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error con archivo:\", row[\"path\"], e)\n",
    "        return np.zeros(1024)  # vector nulo si falla algo\n",
    "\n",
    "\n",
    "df_dev[\"embedding\"] = df_dev.progress_apply(get_embedding, axis=1)\n",
    "\n",
    "# Mostrar primeras filas para verificar\n",
    "df_dev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04521a12-5937-45d8-a1f4-ecc9b92019a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TRAIN_PROTOCOL  = BASE_DIR /\"ASVspoof2019_LA_cm_protocols\\ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    TRAIN_PROTOCOL,\n",
    "    sep=\" \",\n",
    "    header=None,\n",
    "    names=[\"speaker_id\", \"audio_id\", \"unused1\", \"system_id\", \"key\"]\n",
    ")\n",
    "\n",
    "print(\"Protocol loaded. Shape:\", df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323acf60-0e8b-491d-919b-e182c77dd39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_DIR = BASE_DIR / \"ASVspoof2019_LA_train/flac/\"\n",
    "\n",
    "def make_path(audio_id):\n",
    "    return os.path.join(TRAIN_AUDIO_DIR, audio_id + \".flac\")\n",
    "\n",
    "df_train[\"path\"] = df_train[\"audio_id\"].apply(make_path)\n",
    "df_train[[\"path\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ade6e9-2063-4bb8-9bfa-7f3314a94453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"exists\"] = df_train[\"path\"].apply(os.path.exists)\n",
    "df_train[\"exists\"].value_counts()\n",
    "label_map = {\"bonafide\": 0, \"spoof\": 1}\n",
    "df_train[\"label_num\"] = df_train[\"key\"].map(label_map)\n",
    "print(df_train[\"label_num\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeddfe4-374e-4532-afc6-0600bce058bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar GPU\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def extract_embedding(path):\n",
    "    # Leer audio\n",
    "    speech, sr = torchaudio.load(path)\n",
    "    speech = speech.squeeze()\n",
    "\n",
    "    if sr != 16000:\n",
    "        speech = torchaudio.functional.resample(speech, sr, 16000)\n",
    "\n",
    "    # Procesor a GPU\n",
    "    inputs = feature_extractor(\n",
    "        speech.numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Enviar inputs a GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward pass sin gradiente\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Mean pooling\n",
    "    emb = torch.mean(outputs.last_hidden_state, dim=1).squeeze()\n",
    "\n",
    "    # Regresar emb a CPU como numpy\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "# Extraer embeddings masivamente\n",
    "df_train[\"embedding\"] = [\n",
    "    extract_embedding(path) for path in tqdm(df_train[\"path\"])\n",
    "]\n",
    "\n",
    "print(\"Embeddings TRAIN listos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c444c-947e-4cc4-96ba-731c9695b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(df_train[\"embedding\"].values)\n",
    "y_train = df_train[\"label_num\"].values\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837154b-eb3a-43e2-83bd-2347d344a71c",
   "metadata": {},
   "source": [
    "\n",
    "Para predecir utilizaré una máquina de soporte vectorial con Kernel de Función de Base Radial\n",
    "* Los embeddings están en $\\mathbb{R}^{1024}$\n",
    "* La separacion entre bonafide vs spoof no es perfectamente lineal (o eso espero)\n",
    "* RBF kernel permite fronteras no lineales suaves\n",
    "* los embedings están centrados y normalizados por diseño dle modelo\n",
    "* el número de muestras es moderado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936eacbc-d755-4b6b-8ffc-3c449ec6196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convertir embeddings y etiquetas a matrices numpy\n",
    "\n",
    "X_dev = np.vstack(df_dev[\"embedding\"].values)\n",
    "y_dev = df_dev[\"label_num\"].values\n",
    "\n",
    "X_dev.shape, y_dev.shape\n",
    "## debe salir ((N,1024),(N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44056f71-9bf6-41ee-8f3c-9c610b00e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_dev.npy\", X_dev)\n",
    "np.save(\"y_dev.npy\", y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6310c59-1a42-4a7c-90f0-6f33d76bf9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1403b-dc7e-4145-969a-36c5209bfccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee58e4-9291-40fa-9472-4551877d6cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79542384-c8be-4479-8668-c63afed9e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d921ea8-7adc-4ffb-a7fe-887f0e4f5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834aca3-7f94-4bb7-bc51-f04eb5133dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
